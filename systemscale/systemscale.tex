\chapter{System-Scale Data Locality Management}
\label{ch:systemscale}
%% EJ : Here is what I remember we decided at the meeting: 
%% There are 3 points : 
%% 1) Topology-mapping
%% 2) storage and data
%% 3) Resource management and batch scheduler


%% This section should contain an overview of the different problematics and a
%% description of existing solutions and open problems. 





HIGHLIGHTS

\begin{itemize}

\item SCOPE

\begin{itemize}
\item topology mapping
\item storage and data
\item Resource management and batch scheduler
\end{itemize}

\item KEY POINTS

\begin{itemize}
\item However the application is written and optimize, the way it is executed
  has a huge impact on its performance
 \item Mapping of application to resources is important even in the case of
   contiguous allocation. But joint allocation and mapping  (application
   expresses its need and the batch scheduler tries to fulfill it) provides
   better optimization
\item there is a need for models of the machine at different scales (cache, node,
memory, network, etc.) Such models are necessary to design algorithms to enhance
performance
  \item Usually, each part of the application ecosystem (storage, runtime, batch
    scheduler, etc.) act independently. There is a need for joint decisions both
    vertically (from node to storage) and horizontally (e.g. between compute
    nodes)
\item need model of application (same reason as above)
\begin{itemize}
\item shift in application communication models from BSP to over-decomposition model
\item graph-based analytic applications
\item applications using stencil communication patterns
\item applications using transpose communication patterns
\end{itemize}
\item \ldots
\end{itemize}

\item RELATED WORK

\begin{itemize}
\item Topology mapping: TreeMatch, Libtopomap, MPIPP? scotch, metis  
\item Geometric mapping: Zoltan2
\item Storage: parallel file system (???)
\item resource management: queuing systems 'SLURM, LSF, PBS, etc.)
\item machine models: HWLOC, NETLOC, etc. 
\item building app model: profilers (scalasca), compilers, runtime monitoring (openMPI), 
miniApps, etc. 
\end{itemize}

\item CHALLENGES

\begin{itemize}
\item scalable management of the hierarchy and topology
\item globality (tackle the problem system wide)
\item interaction between different layers
\item alternative node architectures
\item alternative topologies, bottlenecks
\item validation of models
\item \ldots
\end{itemize}

\end{itemize}

\section{Introduction}
%% An idea of the presentation 
 

Parallel computers are becoming more and more complex.  Indeed, they
feature hundreds of thousands of cores, a deep memory hierarchy with
several cache layers, non-uniform memory access with  several levels of
memory (flash, non-volatile, RAM, etc), elaborate topologies (both at
the shared-memory level and at the distributed-memory level using
state-of-the-art interconnection networks) and advanced parallel storage
systems and resource management tools.

Nowadays it is already more challenging to write an application that
efficiently accesses its data than one that efficiently processes the
data. In other words arranging bytes is more complex than computing
flops. As it is expected that the memory per core will decrease in the
coming years this problem will become even more important. 

To address the data locality issue system-wide, one approach consists of
working on the application ecosystem and how the application is executed
(interaction with the batch-scheduler and the storage, optimization on
the way the application uses the resources, its relation with the
topology, the interaction with other executing application, etc.). 

More precisely, the important issue lies in the strong need for new
models and algorithms, new mechanisms and tools for improving (1)
topology-aware data accesses, (2) data movements across the various
software layers, (3) data locality and transfers for applications. 


\section{Key points}
In the literature there are tremendous efforts for optimizing an application
statically (better data layout, compilation optimization, parallelism
structuring, etc.). However, once an application is written and compiled there
are several factors that play an important role, that could not be known before
the execution and have a dramatic impact on its performance. Among these
factors are:
\begin{itemize}
\item The allocated resources for the specific run and their interconnection,
\item the other running applications and the network traffic they induce,   
\item the topology of the target machine,
\item the data accessed by the application and their location on the storage
  system,
\item dependencies of the input on the execution,
\end{itemize}
and many more.

It is important to remark that these runtime factors are orthogonal to
the static optimizations that can be performed. In other words, no
matter how the application is written and optimized, the way it is
executed and the interaction with its ecosystem have a huge impact on its
behavior. For instance, it has been shown that  the way resources are
allocated to an application plays a crucial role in the performance of the
execution.  Recent works~\cite{cui2013accelerating,kramer13} have
demonstrated that a non-contiguous allocation can slowdown the
performance by more than 30\%. However, a batch scheduler cannot always
provide a contiguous allocation and even in the case of such allocation
the way processes are mapped to the allocated resources have a big
impact on the performance~\cite{DBLP:conf/ics/ChenCHRK06,jm10}. The
reason is often the complex network and memory topology of modern HPC
systems and that some pairs of processes exchange more data than some
other pairs. 

Futhermore, energy constraints imposed by exascale goals are altering
the balance of interconnect capabilities, reducing the bandwidth to
compute ratio while increasing injection rates.  This shift is causing
fundamental reconsideration of the BSP programming model and
interconnect design.  A leading contender for a new interconnect is a
multi-level direct network such as
Dragonfly~\cite{4556717,ibm-percs-network}.  Such networks are formed
from highly-connected parts, placing every node within a few hops of all
others.  This may benefit unstructured communications that often occur
in graph
algorithms, but limited connections between parts can be bottlenecks for
structured communication patterns and the network topologies have not been 
completely specified.  
At the node level, a promising approach for fully utilizing
higher core counts on next-generation architectures is over-decomposed
task parallelism, which will stress the interconnect in ways different
from the traditional BSP model.


In order to optimize system-scale application execution we need models
of the machine at different scales, models of the application and algorithms
and tools to optimize its execution with its whole ecosystem. 
The literature provides a lot of models and abstraction on how to write a
parallel code. However, even with a huge parallelism, future applications will
not scale due to the data traffic and coherence management. Current models and
abstraction are geared towards computations and ignore the cost incurred by data
movement, topology and synchronization.  It is important to provide new hardware
models to account for these phenomena as well as abstractions to enable the
design of efficient topology-aware algorithms and tools. 

A hardware model is needed to control locality.  Modeling the future large-scale
parallel machines will require work in the following directions: (1) better
describe the memory hierarchy (2) provide an integrated view with the nodes and
the network (3) exhibit qualitative knowledge and, (4) provide ways to express
the multi-scale properties of the machine.

Applications need abstractions allowing them to express their behavior
and requirement in terms of data access, locality and communication.
For this, we need to define metrics to capture the notions of data
access, affinity, network traffic, etc. The MPI standard offers the
process topology interface that allows an application to specify the
dataflow between processes~\cite{hoefler-mpi-2.2-scal-topo}. However,
while this interface is a good first step, it is essentially limited to
BSP-style applications.
%
To optimize execution at system scale, we need to provide mechanisms,
tools and algorithms that are based on the environment (given by the
network model) and the application requirements (given by application
models and abstractions). Based on that, several optimizations can be
performed such as: improving storage access, mapping processes onto
resources based on their
affinity~\cite{hjm14,DBLP:conf/ics/HoeflerS11,Navauxandal2009},
selecting resources according to the application communication pattern
and the pattern of the currently running applications. It is also
possible to couple allocation and mapping.


\section{State of the Art}
Concerning topology mapping TreeMatch~\cite{jmt14b}, provides mapping
of processes onto computing resources in the case of a tree topology (such as
current NUMA nodes and fattree
network). LibTopoMap~\cite{DBLP:conf/ics/HoeflerS11} addresses the same problem as
TreeMatch but for arbitrary topology such as torus, grid, and others. Topology mapping is basically a graph embedding problem 
where an application graph is embedded into a machine graph.  Therefore, graph partitioners 
such as Scotch~\cite{scotch-man} or ParMetis~\cite{karypis2003parmetis} can address the
problem even though they might require more precise information than specific
tools and do not always provide good solutions~\cite{jmt14b}.  
After processes are allocated to an application, Zoltan2~\cite{zoltan2,drl+14} is a 
toolkit that can map processes to the allocated resources depending on the geometry of the 
target machine and process affinity.


Hardware Locality (HWLOC)~\cite{hwloc} is a library and a set of tools aiming at
discovering and exposing the hardware topology of machines, including
processors, cores, threads, shared caches, NUMA memory nodes and I/O devices.
NETLOC~\cite{netloc} is a network model extension of HWLOC to account for locality
requirements of the network. For instance, the network bandwidth and the way
contention is managed may change the way the distance within the network is
expressed or measured. 

Modeling the data-movement requirements of an application in terms of
network traffic and I/O can be supported through performance-analysis tools
such as Scalasca~\cite{geimer_ea:2010:scalascaarchitecture}. It can also be done
by tracing data exchange at the runtime level with a system like OVIS \cite{SystemMonitoring,ovis}, by monitoring the
messages transferred between MPI processes for instance. Moreover, compilers, by analyzing
the code and the way the array are accessed can, in some cases, determine the
behavior of the application regarding this aspect. 

Resource managers or job scheduler, such as SLURM~\cite{yoo2003slurm},
OAR~\cite{capit2005batch,}, LSF~\cite{zhou1992lsf}
or PBS~\cite{henderson1995job} have the role to allocate resources for executing the
application. They feature technical differences but basically they offer the
same set of services: reserving nodes, confining application, executing
application in batch mode, etc. However, none of them is able to match the
application requirements in terms of communication with the topology of the
machine and the constraints incurred by already mapped applications. 


{\bf TODO:}  
\begin{itemize}
\item Storage (e.g. parallel file system (???)), etc. 
\end{itemize}

\section{Discussion}
To address the locality problem at system scale, several challenges are required
to be solved. 

First, scalability is a very important cross-cutting issue since the targets are
very large-scale, high-performance computers. On one hand, applications
scalability will mostly depends on the way data is accessed and locality is
manage and, on the other hand, the proposed solutions and mechanisms have to run
at the same scale of the application and their inner decision time must
therefore be very short.

Second, it is important to tackle the problem for the whole system: taking into account
the whole ecosystem of the application (storage, resource manager, etc.) and the
whole architecture (i.e., from cores to network). It is important to investigate
novel approaches to control data locality system-wide, by integrating
cross-layer I/O stack mechanisms  with cross-node topology-aware mechanisms. 

Third, most of the time, each layer of the software stack is optimized
independently to address the locality problem. However, some optimizations can
be conflicting. It is required to identify how the different approaches interact
with each-other and propose integrated solutions that provide a global
optimizations crossing the different layers.

Ultimately, the validation of the models and solutions to the key points and challenges above 
will be a key challenge.

\section{Research Plan}

To solve the above problems, we propose co-design between application 
communication models, specific network structures, and algorithms for 
allocation and task mapping.
We will determine the effect of over-decomposition on application 
communication.
To benefit structured communication patterns on multi-level networks, we will 
complete the topology (specifying pairs of nodes to be connected) and 
techniques to optimize allocations and task mappings.
