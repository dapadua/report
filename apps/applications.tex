\chapter{Motivating Applications and Their Requirements}
\label{ch:apps}

% define command for HIPAcc acronym
\newcommand{\hipacc}{\textsf{HIPA\nolinebreak[4]\hspace{-0.1em}\textsuperscript{cc}}}


\marginparsep 7pt % restore marginpar sep
\marginparwidth 5.5pc % restore marginpar width
\colorlet{fhcolor}{ProcessBlue}
\newcommand{\TODO}[3]{\todo[#1]{\sffamily\footnotesize\textbf{\textcolor{black}{#2:\,}}\textcolor{black}{#3}}\xspace}
\newcommand{\fh}[2][]{\TODO{color=fhcolor!40,#1}{Frank}{#2}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% S U M M A R Y   P O I N T S
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \noindent
% {\large\textbf{SUMMARY POINTS}}

% $ $\\
% \noindent
% \textbf{Agreements}
% \begin{itemize}
%   \item Non negotiable: A stable program paradigm with a life cycle that is at least several times that of the development cycle for application codes
%     \begin{itemize}
%       \item Applications are leery of adding dependencies
%       \item Code transformation to another high level language may be suboptimal, but maybe more attractive to apps
%       \item Apps are less skeptical of forms of embedded notations (DSLs or libraries) than full languages
%     \end{itemize}
%   \item Application development often locks in data structure before fully fleshing out the algorithm.
%     A formalism that imposes the discipline of algorithm before the data-structure might result in more opportunity for data locality in the app
%     \begin{itemize}
%       \item In general constraining semantics is desirable as long as the users can override the constraints when they really need it
%       \item Inling assembly in performance critical sections is still practiced
%     \end{itemize}
%   \item Data models are often very clear for the application, but it is not always possible to express them as a programming construct
%   \item (This is not yet a general agreement, but I hope people can be persuaded)
%     An application with more than one model needs a composable framework that can stitch together diverse data management demands placed upon the machines by different algorithms.
%     (We agree that we need composable applications, but not that frameworks are the obvious choice to do that)
%   \item Tool-chain supporting the incremental migration to the new programming paradigms are extremely important 
%     \begin{itemize}
%       \item In order to verify the correctness in the intermediate stages of migration  
%       \item Also in general
%     \end{itemize}
% \end{itemize}


% \noindent
% \textbf{Disagreements}
% \begin{itemize}
%   \item Whether frameworks are the answer: can they handle really divergent sets of models in the same code
%   \item Users appear to be voting with their feet by using python and matlab, should we be even concerned with new languages
%     \begin{itemize}
%       \item there are limitations to using those models for HPC
%       \item They use low level libraries and high level constructs for composability, maybe that is a workable model?
%     \end{itemize}
% \end{itemize}

% \noindent
% Things to Ponder
% \begin{itemize}
%   \item Communication avoidance algorithms are essentially a way of horizontal caching (as opposed to vertical caching in memory
%   hierarchy)
%   \item Should be able to express locality in the data structure, something that not only does not exist now, but also there is no theory for
%   it.
%     In fact there is no theory for data movement. (I think this point is extremely important)
%   \item Should caching and redundant computations co-exist ? YES
%   \item Would scratch-pads be better than caches (possibly)
%   \item What is the optimum level of abstraction (translate a PDE into a solver, which is clearly intractable, or abstractions at the level
%    of a functional module (probably still too ambitious) or something lower
%   \item The chicken-and-egg-problem of DSL/language design-is it possible to create something general enough that can become its own open source
%    community?
% \end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% S K E L E T O N
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
$ $\\
\noindent
Discussion of data locality from the perspective of applications implies
consideration of the range of modeling methods used for exploring
scientific phenomena. Even if we restrict ourselves to only a small
group of scientific applications, there is still a big spectrum to be
considered. We can loosely map the applications along two dimensions:
spatial connectivity, and componentization. Spatial connectivity has
direct implication on locality: the left end of this axis represents
zero-connectivity applications which are embarrasingly parallel and at
the right end are the ones with dynamic connectivity such as adaptive
meshing with static meshes falling somewhere in-between. The componentization
axis is more concerned with software engineering where the left end
consists of the small static codes, while at the right end are the
large-multicomponent  codes with a continuous development-debugging
process. The HPC applications space mostly occupies the top right
quadrant, and was the primary concern in this workshop. 

While the applications space itself is very
large, the participating applications and experts provided a good
representation of the numerical algorithms and techniques used in
majority of state-of-the-art application codes (i.e.\ COSMO\cite{cosmo},
GROMACS\cite{gromacs4,gromacs4.5,gromacs-exascale}, Hydra \&
OP2\cite{Hydra, op2}, CHOMBO\cite{chombo}, VIS\cite{vis}. 
Additionally, because several of them model
multi-physics phenomena with several different numerical and
algorithmic technologies, they highlight the challenges of
characterizing the behavior of individual solvers when embedded in a
code base with heterogeneous solvers. %  Because of the presence of many
% tightly coupled solvers 
These applications also demonstrate the importance of interoperability
among solvers and libraries. The science domain which rely on multiphysics modeling
include many physical, biological and chemical systems, e.g.\ climate
modeling, combustion, star formation, cosmology, blood flow, protein
folding to name only a few. The numerical algorithms and solvers
technologies on which these very diverse fields rely include
structured and unstructured mesh based methods, particle methods,
combined particle and mesh methods, molecular dynamics,  and many
specialized 0-dimensional solvers specific to the domain. 

Algorithms for scientific computing vary in their degree of
arithmetic intensity and inherent potential for exploiting data locality.
\begin{itemize}
\item For example, GROMACS short-ranged non-bonded kernels treat
all pairwise interactions within a group of particles, performing
large quantities of floating-point operations on small amounts of
heavily-reused data, normally remaining within lowest-level cache.
% MJA - we can lose ``, normally... cache.'' if this is too long
Exploiting data locality is almost free here, yet the higher-level
operation of constructing and distributing the particle groups
to execution sites, and the lower-level operation of scheduling
the re-used data into registers, require tremendous care and
quantities of code in order to benefit from data locality at those
levels.
% MJA - happy to lose these next sentences
The long-ranged global component can work at a low resolution,
such that a judicious decomposition and mapping confines
a fairly small amount of computation to a small subset
of processes. This retains reasonable data locality, which
greatly reduces communication cost.
\item By contrast, typical low-order partial differential equation
  solvers typically   have few operations per data item even with
  static meshing and therefore struggle with achieving a high degree
  of data reuse. Adaptivity in structured AMR (i.e. Chombo) further
  reduces the ratio of arithmetic operations to data movement by
  moving the application right along the connectivity
  axis. Unstructured meshes have an additional   layer of indirection
  that exacerbates this problem.
\item Multiphysics applications further add a
  dimension  in  the data locality challenge; that of the 
different data access patterns in different solvers. For example in
applications where particles co-exist with AMR, the distribution of
particles relative to the mesh needs to balance spatial proximity with
load balance, especially if the particles tend to cluster in some
regions. 
%AD - can we get some text here from climate ?
\end{itemize}

There is well known
and valid concern in the applications communities about wise
utilization of the scarcest of the resources, the developers' time, and
protecting the investment already made in the mature production codes
of today. The most important consideration for the applications
community, therefore, is the time-scale of change in paradigms in the
platform architecture and major rewrites of their codes. A stable
programming paradigm with a life-cycle that is several times the
development cycle of the code must emerge for sustainable science. The
programming paradigm itself can take any of the forms under
consideration, such as domain-specific languages, abstraction
libraries or full languages, or some combination of these. The
critical aspects are the longevity, robustness and the reliability of
tools available to make the transition. 
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% T h e   S t a t e - o f - t h e - A r t
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The State-of-the-Art}
% \begin{itemize}
%   \item Those that don't have their heads in the sand are experimenting with techniques under consideration for exascale
%     \begin{itemize}
%       \item Frank's DSL (\hipacc~\cite{MHTKE12a}), OP2 for unstructured meshes, microblocking in AMR, DSLs in Cosmo, SIMD for non-bonded iterations or ensemble
%      simulations in Gromacs 
%     \end{itemize}
%   \item Hampered in their efforts by not having a stable paradigm to program to 
%     \begin{itemize}
%       \item Resorting to boutique solutions with varying degrees of success
%       \item OK for the near term, but unsustainable for the long term
%     \end{itemize}
% \end{itemize}

Among the domain science communities relying on modeling and
simulation to obtain results, there is huge variation in awareness and
preparedness for the ongoing hardware platform architecture
revolution. Different research groups vary in the extent to which they
need to prepare. A great deal of useful science is still done by a
small research group with prototype-level software of very limited scope.
Like all other software, these too will
need to be refactored or rewritten for different platforms, but being
small, they are unlikely to be enough of a drain on resources to warrant
looking for general portable solutions. Those research efforts were
not the focus of the workshop, even though they too will gain from more
stability. Instead, the purpose of the workshop was to consider those
computation-based science and engineering research efforts that rely
upon larger codes with many moving parts that also  demand more
resources to compute one single model. Many times, the process of
scientific discovery requires exploration of the parameter space with
a corresponding need to compute several models. For such applications,
efficient, sustainable and portable scientific software is an absolute
necessity, though not all practitioners in these communities are
cognizant of either the extent or urgency of the need for rethinking
their approach to software. Even those that are fully aware of the
challenges facing them have been hampered in  their efforts to find
solutions because of a lack of a stable paradigm that they can adopt.   
% insert some text here about what do apps want out of a program

The research communities that have been engaged in the discussions
about peta- and exa-scale computing are well informed about the
challenges they face.
\textcolor{fhcolor}{Many have started to experiment with approaches
recommended by the researchers from the compilers, programming
abstractions and runtime systems communities in order to be better
prepared for the platform heterogeneity.}\fh{Sentences that I
changed / added, are marked in blue.}

At the workshop examples of many such efforts were presented.
\textcolor{fhcolor}{These efforts can be mainly classified into two
classes: (1) Approaches based on Domain-Specific programming Languages
(DSL) and (2) library-based methods.}

\textcolor{fhcolor}{For example \hipacc~\cite{MHTKE12a}, OP2~\cite{}\fh{I think OP2 is an active library for solving unstructured mesh-based applications, and PyOP2 is the DSL on top of it---I'll check and refine}, and Stella~\cite{} showed
\emph{embedded domain-specific languages} (\textbf{$<$reference to appropriate place in Ch.~4 ``Language \ldots''$>$}) being used to good effect in their target science
domains.}
Other efforts, such as use of tiling within patches in AMR for
exposing greater parallelism rely on a library-based approach.
Many of these efforts
have met with some degree of success. Some are in effect a usable
component of an overall solution to be found in future, while others
are experiments that are far more informative about how to rethink the
data and algorithmic layout of the core solvers. None of them provide
a complete stable solution that applications can use for their
transition to long term viability. 

% MJA - happy to trim this if we think it's too much detail for an example
Viewed on a multi-year time-scale, GROMACS has re-implemented
all of its high-performance code several times, always to make
better use of data locality,\cite{gromacs4,gromacs4.5,gromacs-exascale}
and almost never able to make any use of existing portable high-performance
external libraries or DSLs. Many of those internal efforts have since been
duplicated by third parties with general-purpose, re-usable code,
that GROMACS could have used if they been available at the time.
The latest such efforts in GROMACS have moved the code from
hand-tuned, inflexible assembly CPU kernels to a form implemented
in a compiler- and hardware-portable SIMD intrinsics layer developed
internally, for which the code is generated automatically for a wide
range of model physics and hardware, including accelerators.
In effect, this is an embedded DSL for high-performance short-ranged
particle-particle interactions. If such a DSL could be made widely
available for the molecular simulation community, then a great deal of
duplicated effort could be avoided, and new applications would
benefit immediately.
% AD- it would be nice to have a counter example here with Stella's
% use in Cosmo.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% T h e   C h a l l e n g e s
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{The Challenges}
% \begin{itemize}
%   \item Applications are leery of adding dependencies
%     \begin{itemize}
%       \item Code transformation to another high level language may be suboptimal, but maybe more attractive to apps
%       \item Apps are less skeptical of forms of embedded notations (DSLs or libraries) than full languages
%     \end{itemize}
%   \item Data models are often very clear for the application, but it is not always possible to express them as a programming construct
%   \item Should be able to express locality in the data structure, something that not only does not exist now, but also there is no theory for
%   it.
%     In fact there is no theory for data movement. (I think this point is extremely important)
%   \item Application development often locks in data structure before fully fleshing out the algorithm.
%     A formalism that imposes the discipline of algorithm before the data-structure might result in more opportunity for data locality in the app
%     \begin{itemize}
%       \item In general constraining semantics is desirable as long as the users can override the constraints when they really need it
%       \item Inling assembly in performance critical sections is still practiced
%     \end{itemize}
%   \item The chicken-and-egg-problem of DSL/language design-is it possible to create something general enough that can become its own open source
%    community?
% \end{itemize}

\section{The Challenges}
There are many factors that affect the decision by the developers of a
large scientific library or an application code base to use an 
available programming paradigm, but the most important one is fear of
adding a critical dependency that may not be supported in the long
term, or is portable now and in the future. Often the developers
will opt for a suboptimal or custom-built
solution that does not get in the way of being able to run their
simulations. For this reason embedded DSLs, or code transformation
technologies are more attractive to the applications. These
techniques, because they are not all-or-none solutions, have the added
advantage of providing an incremental path for transition. The trade-off
is the possibility of missing out on some compiler-level optimizations.  

There are other less considered but possibly equally critical concerns
that have to do with expressibility. The application developers can
have a very clear notion of their data model without finding ways of
expressing them effectively in the available data structures and
language constructs. The situation is even worse for expressing the
latent locality in their data model to the compilers or other
translational tools. None of the prevalent mature high-level languages
being used in scientific computing have constructs to provide means of
expressing data locality. There is no theoretical basis for the
analysis of data movement within the local memory or remote
memory. Because there is no formalism to inform the application
developers about the implications of their choices, the data
structures get locked into the implementation before the algorithm
design is fully fleshed out.  The typical development cycle of a
numerical algorithm is to focus on correctness and stability first,
and then performance. By the time performance analysis tools are
applied, it is usually too late for anything but incremental
corrective measures, which usually reduce the readability and
maintainability of the code. A better approach would be to model the
expected performance of a given data model before completing the
implementation, and let the design be informed by the expected
performance model throughout the process. Such a modelling tool
would need to be highly configurable, so that its conclusions might
be portable across a range of compilers and hardware, and valid
into the future, in much the same way that numerical simulations
often use ensembles of input-parameter space in order to obtain
conclusions with reduced bias.

% still have to do a paragraph on inline assembly Vs open source DSL
% MJA - this could be tied in a new subsection with GROMACS para above?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% T h e   W i s h   L i s t
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{The Wish List}
% \begin{itemize}
%   \item Non negotiable: a stable program paradigm with a life cycle that is at least several times that of the development cycle for
%    application codes
%   \item Should be able to express locality in the data structure, something that not only does not exist now, but also there is no theory for
%   it.
%     In fact there is no theory for data movement.
%     (I think this point is extremely important)
%   \item Tool-chain supporting the incremental migration to the new programming paradigms are extremely important 
%     \begin{itemize}
%       \item In order to verify the correctness in the intermediate stages of migration
%       \item Also in general
%     \end{itemize}
% \end{itemize}

\section{Application requirements}
As mentioned earlier, for the production-grade applications, the
biggest non-negotiable requirement is the stability of the paradigm.
Code bases that have years of investment in algorithmic development will not
adopt technologies that can prevent them from utilizing computational
resources as they become available. 

For example, even today research groups exist that refuse to use
parallel I/O libraries, despite the fact that these have been robust
and supported quickly on new platforms for several years. They still
prefer to roll their own I/O solution with all the attendant
performance and maintenance overhead just to avoid the dependency. It
would be even harder to persuade these communities to adopt higher-level
abstractions unless there was a bound on the dependency through
the use of a library that was implemented in a highly portable way,
and easy to install and link.

Most languages provide standard containers and data structures
that are easy to use in high-level code, yet very few languages
or libraries provide interfaces for the application developer to
inform the compiler about expectations upon data locality, data layout,
or memory alignment. In the list below we outline some abstractions
and/or language constructs that would be helpful to applications.
 % For example, it ought to be possible to program
% in arrays of structures, and have a full or partial transformation
% to structures of arrays take place automatically if that would
% suit the hardware - or at least to make such an experiment
% possible without a full re-write of the application module.

% MJA - this sentence was hanging in its former paragraph - to what ``model'' does it refer?
% With this model, the code
% can always run in the high level language, while better translation
% tools can provide performance as they become available.

\subsection{The Wish List}
\begin{itemize}
\item Ability to write dimension independent code easily.
\item Depending upon the data access preference of the target
  architecture, switch the data structures between array-of-structures and
  structure-of-arrays without having to rewrite the code.
\item The ability to map abstract processes to given architectures,
  and coupled to this, the ability to express these mappings in either
  memory-unaware or at least flexible formulations. 
\item More, and more complex, information has been demanded from
  modelers that is not relevant to the model itself, but rather to the
  machines. Requiring less information, and allowing
  formulations close to the models in natural language, is a critical
  point for applications. This does not restrict the possible
  approaches, quite the opposite: for example, well engineered
  libraries that provide the memory and operator mechanics can be very
  successful, if they provide intelligent interfaces. DSL approaches should try
  to reverse this direction.  
\item Source-to-source transformations for mapping from high level
  language to low level language.  If tools were made available for
  creating DSLs or language extensions, this approach could even allow
  communities to develop their own domain specific language without
  having to build the underlying infrastructure.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% R e s e a r c h   A r e a s
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Research Areas}
% \begin{itemize}
%   \item What should a multi-component application framework look like in order to maximize data locality in view of diverse and conflicting
%    demands of data access patterns.
%   \item If formalism existed for defining data movement, how can the apps be configured to exploit it
%   \item Would scratch-pads be better than caches, or should catching be both vertical and horizontal
%   \item What is the optimum level of abstraction (translate a PDE into a solver, which is clearly intractable, or abstractions at the level of a functional module (probably still too ambitious) or something lower 
% \end{itemize}

Several research areas emerged from the discussions during the
workshop that can directly benefit the applications communities. Some
of them involve research to be undertaken by the application
developers, while others are more applicable to the compilers and
tools communities. The most important area of research for the
applications community is the high level multi-component framework
that can maximize data locality in presence of diverse and conflicting
demands of data movement. The questions to be addressed include:
\begin{itemize}
\item what methodology should be used to determine what constitutes a
component,
\item what degree of locality awareness is appropriate in a
component,
\item what is the optimum level of abstraction in the component-based
design, i.e.\ who is aware of spatial decomposition, and who is
aware of functional decomposition, if it exists,
\item how to design so that
numerical complexity does not interleave with the complexity arising
out of locality management, and
\item how to account for concerns other
than data locality such as runtime management within the framework so
that they do not collide with one another. 
\end{itemize}

From the applications perspective, it is important to understand the
impact of different locality models. For instance, with increase in
the heterogeneity of available memory options it is worthwhile
evaluating whether scratch-pads are more useful to the applications,
or should the additional technology just be used to deepen the cache
hierarchy. Similarly, within the caching model it is important to know
whether adding horizontal caching is a valuable proposition for the
applications.




