\chapter{Motivating Applications and Their Requirements}
\label{ch:apps}

% define command for HIPAcc acronym
\newcommand{\hipacc}{\textsf{HIPA\nolinebreak[4]\hspace{-0.1em}\textsuperscript{cc}}}


\marginparsep 7pt % restore marginpar sep
\marginparwidth 5.5pc % restore marginpar width
\colorlet{fhcolor}{ProcessBlue}
\newcommand{\TODO}[3]{\todo[#1]{\sffamily\footnotesize\textbf{\textcolor{black}{#2:\,}}\textcolor{black}{#3}}\xspace}
\newcommand{\fh}[2][]{\TODO{color=fhcolor!40,#1}{Frank}{#2}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% S U M M A R Y   P O I N T S
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \noindent
% {\large\textbf{SUMMARY POINTS}}

% $ $\\
% \noindent
% \textbf{Agreements}
% \begin{itemize}
%   \item Non negotiable: A stable program paradigm with a life cycle that is at least several times that of the development cycle for application codes
%     \begin{itemize}
%       \item Applications are leery of adding dependencies
%       \item Code transformation to another high level language may be suboptimal, but maybe more attractive to apps
%       \item Apps are less skeptical of forms of embedded notations (DSLs or libraries) than full languages
%     \end{itemize}
%   \item Application development often locks in data structure before fully fleshing out the algorithm.
%     A formalism that imposes the discipline of algorithm before the data-structure might result in more opportunity for data locality in the app
%     \begin{itemize}
%       \item In general constraining semantics is desirable as long as the users can override the constraints when they really need it
%       \item Inling assembly in performance critical sections is still practiced
%     \end{itemize}
%   \item Data models are often very clear for the application, but it is not always possible to express them as a programming construct
%   \item (This is not yet a general agreement, but I hope people can be persuaded)
%     An application with more than one model needs a composable framework that can stitch together diverse data management demands placed upon the machines by different algorithms.
%     (We agree that we need composable applications, but not that frameworks are the obvious choice to do that)
%   \item Tool-chain supporting the incremental migration to the new programming paradigms are extremely important 
%     \begin{itemize}
%       \item In order to verify the correctness in the intermediate stages of migration  
%       \item Also in general
%     \end{itemize}
% \end{itemize}


% \noindent
% \textbf{Disagreements}
% \begin{itemize}
%   \item Whether frameworks are the answer: can they handle really divergent sets of models in the same code
%   \item Users appear to be voting with their feet by using python and matlab, should we be even concerned with new languages
%     \begin{itemize}
%       \item there are limitations to using those models for HPC
%       \item They use low level libraries and high level constructs for composability, maybe that is a workable model?
%     \end{itemize}
% \end{itemize}

% \noindent
% Things to Ponder
% \begin{itemize}
%   \item Communication avoidance algorithms are essentially a way of horizontal caching (as opposed to vertical caching in memory
%   hierarchy)
%   \item Should be able to express locality in the data structure, something that not only does not exist now, but also there is no theory for
%   it.
%     In fact there is no theory for data movement. (I think this point is extremely important)
%   \item Should caching and redundant computations co-exist ? YES
%   \item Would scratch-pads be better than caches (possibly)
%   \item What is the optimum level of abstraction (translate a PDE into a solver, which is clearly intractable, or abstractions at the level
%    of a functional module (probably still too ambitious) or something lower
%   \item The chicken-and-egg-problem of DSL/language design-is it possible to create something general enough that can become its own open source
%    community?
% \end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% S K E L E T O N
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
$ $\\
\noindent
Discussion of data locality from the applications perspective implies
consideration of the range of modeling methods used for exploring
scientific phenomena. While the  
applications space itself is very large, the set of
applications/applications experts included in the workshop were fairly 
representative of the numerical algorithms and techniques used in
majority of state-of-the-art application codes (i.e.\ \cite{cosmo},
\cite{gromacs}, \cite{Hydra, op2}, \cite{chombo}, \cite{vis}.
Additionally, among the included applications were those that model
multi-physics phenomena combining several different numerical and
algorithmic technologies into one tightly coupled whole. These
applications demonstrate the challenges of characterizing the behavior
of individual solvers when embedded in a larger code base with
multiple and heterogeneous solvers.  Because of the presence of many
tightly coupled solvers these applications also
highlight the importance of interoperability among solvers and
libraries by implication. The science domain which rely on multiphysics modeling
include many physical, biological and chemical systems, e.g.\ climate
modeling, combustion, star formation, cosmology, blood flow, protein
folding to name only a few. The numerical algorithms and solvers
technologies on which these very diverse fields rely include
structured and unstructured mesh based methods, particle methods,
combined particle and mesh methods, molecular dynamics,  and many
specialized 0-dimensional solvers specific to the domain. 

The highly used algorithms in scientific computing have varying
degrees of arithmetic intensity and potential for data locality
inherent to them. For example, Gromacs has a small working set size
for (? short-range) .... therefore benefits from easily obtained data
locality. Where modeling needs to consider global effects, the
required resolution is low enough that a judicious decomposition and
mapping of the work load can confine these spatially long range
computations to a small subset of processes, thereby retaining 
reasonable data locality. By contrast typical low order partial
differential equation solvers with structured mesh or particles
typically have very few operations per data item and therefore
struggle with achieving a high degree of data reuse. Unstructured
meshes have an additional layer of indirection that exacerbates this
problem.  Others fall somewhere in between. Multiphysics applications
further add a dimension  in  the data locality challenge; that of the
different data access patterns in different solvers. Here, increasing
locality for one solver can decrease it for another, thereby
necessitating carefully considered trade-offs. There are well known
and valid concern among the applications communities about wise
utilization of the scarcest of the resources, the developers time, and
protecting the investment already made in the mature production codes
of today. The most important consideration for the applications
community, therefore, is the time-scale of change in paradigms in the
platform architecture and major rewrites of their codes. A stable
programming paradigm with a life-cycle that is several times the
development cycle of the code must emerge for sustainable science. The
programming paradigm itself can take any of the forms under
consideration, such as domain-specific languages, abstraction
libraries or full languages, or some combination of these. The
critical aspects are the longevity, robustness and the reliability of
tools available to make the transition. 
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% T h e   S t a t e - o f - t h e - A r t
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The State-of-the-Art}
% \begin{itemize}
%   \item Those that don't have their heads in the sand are experimenting with techniques under consideration for exascale
%     \begin{itemize}
%       \item Frank's DSL (\hipacc~\cite{MHTKE12a}), OP2 for unstructured meshes, microblocking in AMR, DSLs in Cosmo, SIMD for non-bonded iterations or ensemble
%      simulations in Gromacs 
%     \end{itemize}
%   \item Hampered in their efforts by not having a stable paradigm to program to 
%     \begin{itemize}
%       \item Resorting to boutique solutions with varying degrees of success
%       \item OK for the near term, but unsustainable for the long term
%     \end{itemize}
% \end{itemize}

Among the domain science communities relying on modeling and
simulation to obtain results there is huge variance in awareness and
preparedness for the ongoing hardware platform architecture
revolution. This huge variance also exists in the need that different
research groups have to be prepared. A great deal of useful science is
still done with very limited scope prototype level software in
use by a small research group. Like all other software, these too will
need to be refactored or rewritten for different platforms, but being
small, they are unlikely to be enough of drain on resources to warrant
looking for general portable solutions. Those research efforts were
not the focus of the workshop even though they too will gain from more
stability. Instead, the purpose of the workshop was to consider those
computation based science and engineering research efforts that rely
upon larger codes with many moving parts that also  demand more
resources to compute one single model. Many times the process of
scientific discovery requires exploration of the parameter space with
a corresponding need to compute several models. For such applications
efficient, sustainable and portable scientific software is an absolute
necessity, though not all practitioners in these communities are
cognizant of either the extent or urgency of the need for rethinking
their approach to software. Even those that are fully aware of the
challenges facing them have been hampered in  their efforts to find
solutions because of a lack of a stable paradigm that they can adopt.   
% insert some text here about what do apps want out of a program

The research communities that have been engaged in the discussions
about peta- and exa-scale computing are well informed about the
challenges they face.
\textcolor{fhcolor}{Many have started to experiment with approaches
recommended by the researchers from the compilers, programming
abstractions and runtime systems communities in order to be better
prepared for the platform heterogeneity.}\fh{Sentences that I
changed / added, are marked in blue.}

At the workshop examples of many such efforts were presented.
\textcolor{fhcolor}{These efforts can be mainly classified into two
classes: (1) Approaches based on Domain-Specific programming Languages
(DSL) and (2) library-based methods.}

\textcolor{fhcolor}{For example \hipacc~\cite{MHTKE12a}, OP2~\cite{}\fh{I think OP2 is an active library for solving unstructured mesh-based applications, and PyOP2 is the DSL on top of it---I'll check and refine}, and Stella~\cite{} represented the use of
\emph{embedded domain-specific languages} (\textbf{$<$reference to appropriate place in Ch.~4 ``Language \ldots''$>$}) being used to good effect in their target science
domains.}
Other efforts such as use of tiling within patches in AMR for
exposing greater parallelism rely on a library based approach. Still
others such as Gromacs using SIMD operations for non-bonded iterations
or doing ensemble simulations are targeted at utilizing vectorization
possibilities presented by the accelerators. Many of these efforts
have met with some degree of success. Some are in effect a usable
component of an overall solution to be found in future, while others
are experiments that are far more informative about how to rethink the
data and algorithmic layout of the core solvers. None of them provide
a complete stable solution that applications can use for their
transition to long term viability. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% T h e   C h a l l e n g e s
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{The Challenges}
% \begin{itemize}
%   \item Applications are leery of adding dependencies
%     \begin{itemize}
%       \item Code transformation to another high level language may be suboptimal, but maybe more attractive to apps
%       \item Apps are less skeptical of forms of embedded notations (DSLs or libraries) than full languages
%     \end{itemize}
%   \item Data models are often very clear for the application, but it is not always possible to express them as a programming construct
%   \item Should be able to express locality in the data structure, something that not only does not exist now, but also there is no theory for
%   it.
%     In fact there is no theory for data movement. (I think this point is extremely important)
%   \item Application development often locks in data structure before fully fleshing out the algorithm.
%     A formalism that imposes the discipline of algorithm before the data-structure might result in more opportunity for data locality in the app
%     \begin{itemize}
%       \item In general constraining semantics is desirable as long as the users can override the constraints when they really need it
%       \item Inling assembly in performance critical sections is still practiced
%     \end{itemize}
%   \item The chicken-and-egg-problem of DSL/language design-is it possible to create something general enough that can become its own open source
%    community?
% \end{itemize}

There are many factors that affect the decision by the developers of a
large scientific library or an application code base to use an 
available programming paradigm, but the most important one is fear of
adding a critical dependency that may not be supported in the long
term. Often the developers will opt for a suboptimal or custom built
solution that does not get in the way of being able to run their
simulations. For this reason embedded DSLs, or code transformation
technologies are more attractive to the applications. These
techniques, because they are not all-or-none solutions, have the added
advantage of providing an incremental path for transition. The trade-off
is the possibly of missing out on some compiler level optimizations.  

There are other less considered but possibly equally critical concerns
that have to do with expressibility. The application developers can
have a very clear notion of their data model without finding ways of
expressing them effectively in the available data structures and
language constructs. The situation is even worse for expressing the
latent locality in their data model to the compilers or other
translational tools. None of the prevalent mature high level languages
being used in scientific computing have constructs to provide means of
expressing data locality. There is no theoretical basis for the
analysis of data movement within the local memory or remote
memory. Because there is no formalism to inform the application
developers about the implications of their choices, the data
structures get locked into the implementation before the algorithm
design is fully fleshed out.  The typical development cycle of a
numerical algorithm is to focus on correctness and stability first
and then performance. By the time performance analysis tools are
applied it is usually too late for anything but the incrementally
corrective measures, which usually reduce the readability and
maintainability of the code. A better approach would be to model the
expected performance of a given data model before completing the
implementation and let the design be informed by the expected
performance model throughout the process. 

% still have to do a paragraph on inline assembly Vs open source DSL

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% T h e   W i s h   L i s t
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{The Wish List}
% \begin{itemize}
%   \item Non negotiable: a stable program paradigm with a life cycle that is at least several times that of the development cycle for
%    application codes
%   \item Should be able to express locality in the data structure, something that not only does not exist now, but also there is no theory for
%   it.
%     In fact there is no theory for data movement.
%     (I think this point is extremely important)
%   \item Tool-chain supporting the incremental migration to the new programming paradigms are extremely important 
%     \begin{itemize}
%       \item In order to verify the correctness in the intermediate stages of migration
%       \item Also in general
%     \end{itemize}
% \end{itemize}

As mentioned earlier, for the production grade applications the
biggest non-negotiable is the stability of the paradigm. Code bases
that have years of investment in algorithmic development will not
adopt technologies that can prevent them from utilizing computational
resources as they become available. There exist research groups even
today that refuse to use parallel IO libraries, which have been robust
and early to get on new platforms for a few years now. They still
prefer to roll their own IO solution with all the attendant
performance and maintenance overhead just to avoid the dependency. It
would be even harder to persuade these communities to adopt higher
level abstractions unless there was a bound on the dependency through
the use of easy to install and link library. With this model the code
can always run in the high level language, while better translation
tools can provide performance as they become available.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% R e s e a r c h   A r e a s
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Research Areas}
% \begin{itemize}
%   \item What should a multi-component application framework look like in order to maximize data locality in view of diverse and conflicting
%    demands of data access patterns.
%   \item If formalism existed for defining data movement, how can the apps be configured to exploit it
%   \item Would scratch-pads be better than caches, or should catching be both vertical and horizontal
%   \item What is the optimum level of abstraction (translate a PDE into a solver, which is clearly intractable, or abstractions at the level of a functional module (probably still too ambitious) or something lower 
% \end{itemize}

Several research areas emerged from the discussions during the
workshop that can directly benefit the applications communities. Some
of them involve research to be undertaken by the application
developers, while others are more applicable to the compilers and
tools communities. The most important area of research for the
applications community is the high level multi-component framework
that can maximize data locality in presence of diverse and conflicting
demands of data movement. The questions to be addressed include: (1)
what methodology should be used to determine what constitutes a
component, (2) what degree of locality awareness is appropriate in a
component, (3) what is the optimum level abstraction in the component
based design, i.e.\ who is aware of spatial decomposition, and who is
aware of functional decomposition it it exists, (4) how to design so that
numerical complexity does not interleave with the complexity arising
out of locality management, and (4) how to account for concerns other
than data locality such as runtime management within the framework so
that they do not collide with one another. 

From the applications perspectives it is important to understand the
impact of different locality models. For instance, with increase in
the heterogeneity of available memory options it is worthwhile
evaluating whether scratch-pads are more useful to the applications,
or should the additional technology just be used to deepen the cache
hierarchy. Similarly within the caching model it is important to know
whether adding horizontal caching is a value proposition for the
applications. 