% !TEX root = ../padalreport.tex
\chapter{Data Structures and Layout Abstractions}
\label{ch:datastructures}


%Define your Area
%  Create definition of your research area
%  Describe key concepts that define your area or that were uncovered during the course of conversation
%  A few examples of work in that area (can refer back to talks on website, but no need to recount entire talk)

%Findings:
%‚ñ† Describe points/observations/discoveries/challenges/issues uncovered in the session
 %‚óè Distill into summary (major discoveries)
 %‚óè can refer back to presentations for details
 %‚óè Can also use data from panel discussions
%‚ñ† Identify areas of agreement
 %‚óè Common approaches
 %‚óè Common concerns
%‚ñ† Identify areas of disagreement
 %‚óè what is the substantive cause of the disagreement (document)
 %‚óè What metrics/information/research are needed to compare/resolve
%‚ñ† Identify Gaps
 %‚óè What is missing?

%Recommendations
%‚ñ† Opportunities for standardization of mature technologies where the is substantial agreement or commonality
 %‚óè Have we met the necessary conditions for standardization (is the area well enough understood, are the elements of existing implementations sufficiently similar, are the benefits clearly demonstrated, is there a user community?)
 %‚óè What should we standardize? ( Low¬≠hanging fruit )
 %‚óè How can we influence standards committees? (e.g. C++17standards committee?)
%‚ñ† Define research agenda for new ideas or areas where there is insufficient information to choose a final implementation ( What areas need more research?)
 %‚óè identify research thrust
 %‚óè what are the opportunities
 %‚óè what needs to be done
 %‚óè What needs to be prioritized?
 %‚óè What resources would be required (estimate size/complexity ofthe problem if you can)
%‚ñ† How do we create a user community? (bonus question)

\section{Introduction}
\note{Scroll down to see the original outline}


Lack of a single abstract machine model  due to the diversity of the architectures 
requires the programmer to support various implementations of the same program to achieve high performance
on different platforms. Recently, a number of programming interfaces such as Kokkos, TiDA, OpenMP
extensions, GridTools, Dash, Array Extensions \cite{all} have arise to give developers more control over 
 data layout and to abstract the data layout itself from the application. 
 In this chapter, we discuss the key points when designing such abstractions, emerging approaches with (im)mature
 solutions, and potential research areas. Here, we focus on  locality management on data-parallel algorithms 
 and leave the discussion on task-oriented abstractions to Chapter~\ref{ch:taskmodels}.

Before going into further discussions, let us define relevant terminology. \note{I sent an email 
for discussion, will update this section based on the responses.}
We consider {\em data structure} as an organized collection of 
datum residing in one or more memory spaces (e.g. main memory, local storage).
{\em Data decomposition} is partitioning of data collection into smaller chunks with the intent of fitting small chunks into certain memory space or introducing more parallelism. 
{\em Data placement} maps each partition of data collection onto memory spaces. 
{\em Data access pattern} is the access order to the data by the computation's algorithm and its execution policy and might be different than storage pattern.
{\em Execution policy (Thread Binding?)} determines how tasks are scheduled and mapped onto data collections. 
{\em Data layout} refers to the policy that decomposes data, uses a certain storage pattern and determines placement of data onto memory spaces, 
typically with the intent of efficient and performant access by a collection of computation.  

\note{There is also data distribution. I am not sure how this is different than data placement. }
  
  
\section{Key Points}
It is encouraging that there is a shift towards a more data-centric programming in the research community. 
We consider the following design principles as important and desired by the application programmers.

\begin{itemize}
\item Separation of concerns is important to maximize expressivity and optimize performance. That principle is applied to the distinction between a logical, semantic specification, and lower-level controls over the physical implementation.
 At the semantic level, scalar work is mapped onto parallel data collections, using logical abstractions of data layout which best promote productivity. Domain experts can also expose semantic characteristics like reference patterns that can inform trade-offs made by tools.
\item Performance tuners, who may be distinct from the domain experts, may exercise control over performance with a set of interfaces which can be distinct from those that specify semantics. 
\item Through these interfaces, they may decompose, distribute and map parallel data collections onto efficient physical layouts with specialized characteristics,

\note{The followings were used to be in the terminology but I think we center the discussion around them}
 \item Memory access pattern: The memory access pattern of a computation is determined by the composition of the computation's algorithm, computation's execution policy, and layout of the computation's data structures.  The quality of a computation's memory access pattern is determined by the time or energy consumed accessing its datum.  

  \item Polymorphic layout: The quality of a computation's memory access pattern may depend upon the architecture of the computer on which it executes.  A data structure has a polymorphic layout if the layout can be changed to improve the memory access pattern without modifying the source code of computations depending upon that data structure.

  \item A data structure (-layout) has good locality (-behavior) with respect to an algorithm if operations on the data structure can be executed efficiently in terms of run-time and/or energy consumption. So locality is not a property of the data structure or layout alone, there is an execution/code component that determines if locality behavior is good.

  \item Algorithmic locality vs. actual/implementation locality: Not sure I would know how to define these terms. What is algorithmic locality of matrix multiplication? 
 \end{itemize}


\section{Emerging Approaches}
Here we may want to discuss Kokkos, TiDA, GridTools etc a little bit (few sentences for each)

\section{Discussions or Challenges ?}

Libraries? 
Flavors of functional programming ?

The choice of base language to implement the mentioned abstractions varies among different groups. 
Some prefer standard languages such as Fortran, C/C++ in order to maximize impact and leverage market breadth of the supporting tool chain (e.g., compilers, debuggers, profilers). Wherever profitable, there can be a push to “redeem” existing languages by amending or extending them, e.g. via changes to the specifications or by introducing new ABIs. It is noteworthy that the ISO C++ standardization committee intends to address performant, on-node parallelism in a future language standard.
C++ meta-programming can cover a significant fraction of the desired capabilities, and that it’s a middle road for being able to implement DSLs that are embedded within standard languages. Specialization can be hidden in template implementation and controlled by template parameters. Even though there are a huge number of applications implemented in Fortran, 
there is a concern about Fortran's lack of extensibility in terms of lambdas and templates. Others believe that specialized languages are required, e.g. to “get us to exascale.” Different language rules may be required to overcome limitations imposed by current languages. For example, C exposes physical data layout, and limits a compiler’s ability to re-layout data. Source to source translators are still subject to language rules, whereas new languages may remove such limitations through abstraction.
In conclusion, there is no single language  to physically implement the layout abstractions 


\section{Research Areas}



\section{OUTLINE}

HIGHLIGHTS

\begin{itemize}
\item SCOPE
  \begin{itemize}
  \item In this chapter, we discuss language and library interfaces that help manage programming abstractions for data locality, particularly for data structures and data layout. 
  \end{itemize}
\item KEY POINTS
  \begin{itemize}
  \item \note{Didem: I think this should appear in the introduction of the entire report.}What are the {\em easy} and the {\em complex} parts of algorithm design? I think the {\em easy} should include parallelism. From the implementation point of view this may be different, but compared with the challenges of locality, parallelism should be considered easy and high level. What is the level of ``algorithmic locality'' that we want to be able to specify? Is local/non-local distinction sufficient?
  \item From the requirements of the applications (and scalability) we focus on data-parallel algorithms (even though we may use task oriented abstractions and runtimes).
  \item The lack of a single abstract machine (programming model) to program for performance imposes the choice of data-structure specific constructs and specific implementations of those for different platforms (for now this is an empirical observation)
  \item \note{not sure I got this item, it seems similar to the item right above}The diversity of the architectures also forces these constructs to work at level of {\em means of combination/composition} (it's not sufficient to provide very efficient primitive operations, like BLAS).
  \item Separation of concerns is important to maximize expressivity and optimize performance.  That principle is applied to the distinction between a logical, semantic specification, and lower-level controls over the physical implementation.
  \item At the semantic level, scalar work is mapped onto parallel data collections, using logical abstractions of data layout which best promote productivity.  Domain experts can also expose semantic characteristics like reference patterns that can inform trade-offs made by tools.
  \item Performance tuners, who may be distinct from the domain experts, may exercise control over performance with a set of interfaces which can be distinct from those that specify semantics.  Through these interfaces, they may 
    \begin{itemize}
    \item decompose, distribute and map parallel data collections onto efficient physical layouts with specialized characteristics, 
    \item map parallel work onto underlying hardware mechanisms for supporting parallelism, and 
    \item exercise control over temporal sequencing of work and movement of data for locality.
    \end{itemize}
  \item Some (im)mature solutions implemented in different languages include: Kokkos, TiDA, OpenMP extensions, GridTools, Dash, Array Extensions
\end{itemize}


\item TERMINOLOGY (define each of these up front)
  \begin{itemize}

 \item Data structure: A organized collection of datum residing in one or more memory spaces. 

  \item Layout (of data structure):
    The mapping of a data structure into one or more memory spaces, typically with the intent of efficient and performant access by a collection of computations.

    Example: A 2D matrix of size 100x100 holding integers is a data structure: a semantic/logical entity in which a programmer thinks. Storing and processing the matrix in a computer requires a layout that maps the 2D matrix onto the 1D address space of the computer in some way. The mapping could be a simple linearization or it could be a more complex blocked (tiled) scheme.

  \item Memory space: A computer has multiple memory resources.  A single computer traditionally has main memory, one or more levels of hardware managed cache memory, and registers; where the main memory is the only memory space explicitly managed by a program.  Advanced architectures have heterogeneous memory for performance that must be managed by computations.  Examples of these memory spaces include CPUs' non-uniform memory access (NUMA) regions, separate GPU memory, and software-managed on-chip memory.

 
  \item Distributed data structure / parallel data structure: not sure we should use either of those terms, but if we do we should make sure what we mean: is the data distributed over multiple nodes/address spaces? is there concurrent access to the same data structure? 
      
  \item \note{Didem: should be part of the report intro} Locality: A measure of distance between datum on a computer.  For example, do the datum reside in the same compute node of a cluster, in the same memory space, in the same cache line?  [I can't think of a direct definition that doesn't require a whole lot of other definitions up front, so how about the following phenomenological definition?]

  \item Execution policy: How a computation's units of work are scheduled and mapped onto data collections. \note{is this the same thing as thread binding?}

  \item Memory access pattern: A computation accesses data structures according to its execution policy. 

  \item Memory access pattern: The memory access pattern of a computation is determined by the composition of the computation's algorithm, computation's execution policy, and layout of the computation's data structures.  The quality of a computation's memory access pattern is determined by the time or energy consumed accessing its datum.  

  \item Polymorphic layout: The quality of a computation's memory access pattern may depend upon the architecture of the computer on which it executes.  A data structure has a polymorphic layout if the layout can be changed to improve the memory access pattern without modifying the source code of computations depending upon that data structure.

  \item A data structure (-layout) has good locality (-behavior) with respect to an algorithm if operations on the data structure can be executed efficiently in terms of run-time and/or energy consumption. So locality is not a property of the data structure or layout alone, there is an execution/code component that determines if locality behavior is good.

  \item Algorithmic locality vs. actual/implementation locality: Not sure I would know how to define these terms. What is algorithmic locality of matrix multiplication? 

  \item Logical, semantic level: programmer expresses the WHAT, exposes OPPORTUNITY in a natural expression; portable
  \item Portable efficiency
  \item Algorithmic locality vs. actual locality or implementation locality
  \item Physical, performance level: CONTROL over HOW, so as to meet performance goals; may be implementation specific
  \item Polymorphic: may take different forms, depending on the abstraction layer or optimization target
  \item Language interface: a spec or API used by a programmer, which may be part of the base language, a compiler-interpreted directive, or a library API [this may need work] 
  \item Language construct: something that can be identified as a {\em keyword} in a language interface and the grammar rules that applies to it.

  \item Binding: mapping from the logical to physical domain, e.g. for storage
  \item {\em What follows should be either filled in or dropped}
  \item control space(use execution space?)
  \item binding  (or execution policy ?)
  \item data layout
  \item data decomposition 
  \item data distribution 
  \item iteration space traversal (avoid using loop traversal, maybe use domain traversal?) maybe : {\em iteration patterns} to be paired with {\em access patterns}?
  \item access type
  \end{itemize}


\item AGREEMENTS
  \begin{itemize}
  \item Abstractions for performance portability are needed: Data layout, tile sizes, memory access patterns need to be tuned when application is moved between machines. [We may want to tweak that list.]
  \item The separation of logical from physical concerns enables:
    \begin{itemize}
    \item Separation of concerns between domain experts and performance tuning experts
    \item Maximal exposure of opportunity, vs. hard coding a particular trade-off
    \item Getting free of enslavement to restrictions or suboptimalities imposed at the logical level
    \item Minimizing, or at least localizing, code modification for control over performance
    \item Polymorphism across targets and abstraction layers
    \end{itemize}
  \item We need a data model to assess the data locality
    \begin{itemize}
    \item We have a model for parallelism but do not have a model for data locality
    \item Optimal trade-offs may shift over time and across target systems.  
    \item Models need to take into account the capabilities and capacities of computational elements, memory structures, and the network fabric 
    \item Models should reflect the consequences of various controls, such as the placement of work near data
    \end{itemize}
  \item It's easier to standardize high-level concepts, suggesting that
    \begin{itemize}
    \item Extensions for controls over how parallel work and data are managed may best be targeted at libraries and language interfaces, rather than base languages.  There's greater freedom for diversity there.
    \item Base languages are a good longer-term target for semantically exposing opportuniteis for parallelism (and locality?)
    \end{itemize}
  \item Low hanging fruit: 
    (1) multidimensional array support (in C and C++) 
    (2) runtime polymorphic data layout: 
    ideally change the layout in runtime (it should always be clear when an operation has performance costs),
    (3) compile time polymorphic layout: change the layout at compile time with minimal code modifications, 
    support layout in the type system 
  \item Performance-related controls pertain to data and to execution policy.  These may vary in their scope and granularity.
    \begin{itemize}
    \item Data controls may be used to manage:
      \begin{itemize} 
      \item Decomposition, which tends to be either trivial (parameterizable and automatic) or not (explicit)
      \item Binding to storage: to a particular type of memory (e.g. read only, streaming), to a phase-dependent depth in the memory hierarchy (prefetching, marking non-temporal), or to memory structures which support different kinds of sharing (SW managed, cached)
      \item Mechanisms for, and timing of, distribution to data space/locality bindings
      \item Data layout
      \end{itemize}
    \item Execution policy controls may be used to manage
      \begin{itemize} 
      \item Decomposition of work, e.g. iterations, hierarchical tasks
      \item Ordering of work, e.g. recursive subdivision, work stealing
      \item Association of work with data, e.g. moving work to data, binding to hierarchical domains like a node, thread or a SIMD lane
      \end{itemize}
    \item These controls may be applied at different scopes and granularities through a varity of mechanisms
      \begin{itemize} 
      \item Data types - global, fine-grained, can vary by call site
      \item Function or construct modifiers - local coarse-grained, can vary within a scope
      \item Environmental variable controls - global policies
      \end{itemize}
    \end{itemize}
  \item lambdas for domain traversal (or iteration space traversal) [is this now adequately covered below?]
  \end{itemize}
  
\item  DISAGREEMENTS
  \begin{itemize}
  \item Binding [could someone flesh out the opposing points of view?] Depending on the level at which a language/library stands (single thread, multicore chip, parallel distributed system) the binding is responsibility of the user or the system. I don't think we agree on what the level of abstraction is right, so we could not agree in the binding, I guess.
  \item support for memory spaces: can be hidden from the programmer or exposed [could someone flesh out the opposing points of view?]  Traditional architectures have hidden memory spaces within node through hardware supported movement of datum through a cache memory hierarchy.  This implicit memory space management strategy has been applied to distributed memory clusters through PGAS programming models and runtimes.  There has is not a consensus on PGAS versus explicit management of datum movement among clusters' compute nodes.  Advanced compute node architectures have heterogeneous memory spaces which must be managed for computational performance.  It is an open question as to whether a programming model can be deployed that implicitly manages these memory spaces and provides sufficient memory access quality.

  \item Choice of language interfaces
    \begin{itemize}
    \item Some prefer standard languages, in order to maximize impact and leverage market breadth of the supporting tool chain (e.g., compilers, debuggers, profilers).  Wherever profitable, there can be a push to ``redeem'' existing languages by amending or extending them, e.g. via changes to the spec or by introducing new ABIs.  It is noteworthy that the ISO C++ standardization committee intends to address performant, on-node parallelism in a future language standard.
    \item Others believe that specialized languages are required, e.g. to ``get us to exascale.''  Different language rules may be required to overcome limitations imposed by current languages.  For example, C exposes physical data layout, and limits a compiler's ability to re-layout data.  Source to source translators are still subject to language rules, whereas new languages may remove such limitations through abstraction.
    \item There's some agreement that C++ metaprogramming can cover a significant fraction of the desired capabilities, and that it's a middle road for being able to implement DSLs that are embedded within standard languages.  Specialization can be hidden in template implementation and controlled by template parameters.
    \item There's some concern about Fortran's lack of extensibility, e.g. in the direction of lambdas, templates and metaprogramming
    \end{itemize}
  \end{itemize}

\item GAPS (what is missing? not covered at the workshop)
  \begin{itemize}
  \item A data model for which data layout is more suitable for which algorithm? or metric for locality
  \item The mentioned distinction between WHAT and HOW is subtle. I think every developer is concerned with WHAT. As library/language developers we want our users to be concerned by certain WHAT that we turn into HOW by stating some lower level WHATs. In my work I can use a PGAS approach (in which locality is expressed as a here-or-there) underneath but hide it altogether to my user. I think we disagree at what level our users should express their programs. We agree that there must be a separation of concerns but not where the separation should be.
  \end{itemize}

\item RESEARCH AGENDA
  \begin{itemize}
\item If we speak of data locality, is a binary local/remote distinction enough or do we need a more fine-grained differentiation? If so, what is the best way to represent and measure this multi-level locality concept. Is "hierarchy" always the right concept and is a tree always a good representation?

\item What about horizontal locality management vs. vertical locality management? Do they require different abstractions or can they be handled in a uniform way?

  \item Identify minimal set of data and execution policy controls
  \item Compare/contrast available options for specifying those controls
  \item Identify gaps and prescribe steps toward closure of those gaps
  \item Converge on a minimally set of (semi-standard) solutions that provide adequate coverage
  \item Prove or disprove that an portable efficiency can be achieved with a single language (in order to prove or disprove that data-structure specific approaches are the only beign possible)
  \end{itemize}

\end{itemize}



