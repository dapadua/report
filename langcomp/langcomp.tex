\chapter{Language and Compiler Support for Data Locality}
\label{ch:langcomp}

%%% STRUCTURE:
%%% At the start, in comment - Notes from the google doc we discussed at the meeting
%%% At the end, in comment - Notes from the two scribes
%%% Outline - modelled on what the datastructures group did

%% NOTES FROM THE GOOGLE DOC WE DISCUSSED AT THE MEETING

%Define your area
% - create definiition
%	To maximise portability we need the user-facing level to be … while 
%	Differentiate languages/compilers/tools from models, libraries, active libraries, embedded DSLs
%	Polyhedral?
%	Identify and recognise that solutions operate at different levels of abstraction and automation (“multiresolution”)
%	Principle: avoid losing information, premature lowering
%	Principle: isolate cross-cutting locality concerns
%	cf rank-independence
% - key examples of work in the area
%	HPF (and Brad’s blog) - vagueness - robust, well-defined, well-understand abstractions 
%	UPC (need to have clear ambitions/capability beyond the low-hanging fruit)
%	Research vehicles can be narrow but a basis for a standard has to have potential for beyond
%	Titanium, Global arrays
%Conflict between local-view vs global view (clarify the difference - 
%* Findings
% - agreement	
%There’s a lot of consensus around multidimensional data/partitioning/slicing, and how to iterate over them (generators, iterators) - parameterisation of layouts.
%There’s potential to expose polyhedral concepts to the programmer/IR, and to pass them down to the backend (eg Chapel’s domains, mappings)
%The back-end model for the target for code geenration and implementation of models/compilers is “missing” - for portable implementation, for interoperability
%While we can do a lot with libraries and template metaprogramming, there is a compelling case for a language-based approach
% - differences
%No consensus on the requirements on intermediate representations and runtime systems
%Do what I say vs do what I mean
%Compositional metadata vs explicit dependence/synchronisation/movement (Brad has words)
%	
%
%* Recommendations:
%  - Opportunities for standardisation of mature technologies
%  - What is this standardisable low-hanging fruit?
%  - how to influence standards?
%  - define research agenda
%Scope out the opportunities for JIT - plenty of compelling need for infrastructure (specialisation, autotuning, …) [[what’s the research agenda/challenge here?]]
%  - how do we create a user community?
%
%* Seek input from key participants, esp speakers from our session
%
%Two-page exec summary, 30 page total.  So we’re talking about 3-5 pages.





%%%% NOTES FROM THE SCRIBES
%
%Anshu:
%
%Q. Why a language rather than library, and paths to make it easier 
%to make your language/library adopted
%
%- They are not mutually exclusive, despite long odds we need to keep striving
%   for languages. 
%- Practical reason - syntax matters, things like inferred type and optimization 
%   opportunities better expressed
%- The biggest challenge is getting adopted
%- Academic languages have more challenge to adoption 
%- Template meta programming covers a good percetage of what a language provides
%- The two DSLs based on Python etc they are libraries developed at first by
%    the domain people 
%- Strategically led by applications, essential to maintain the engagement.
%- In order to engage with users - one has to compete with 
%   everything else available
%- Common backend -- target backend c compiler and then provide
%   an appropriate design, take it down to more explicit form 
%- Source to source translator is preferable also for greater portability
%
%- Adoption problem - there is a real hunger for a new language so no excuse for
%   not developing one
%- CUDA- adoption - more than a million downloads, even with small changes
%    timing is important, the real complaint is it only runs on nvidia 
%- Complete rewrite is a red-herring
%
%- The whole tool chain is important,  and interoperability is the key 
%
%Q. What is missing from a common backend 
%
%- targetting C- as a good portable assemble language 
%- 1D array limitation, there is a movement for 
%    support of multi-dimension arrays in C++
%- parallelization/vectorization information
%- lack of any locality information C or C++ how to address it
%- locality is useful even for exploiting memory hierarchy
%- C++ standards committee- all volunteer -- you join ISO pay the fee
%   and then you join the committee and then you attend a few meeting
%   to earn the voting rights
%
%- Locality info is not just hierarchy or also adjacency (helpful for stencils)
%- polyhedral - could be the intermediate information - but the info
%- from the source is lost. How to get this information from the source
%
%- synergy between iteration spaces and domains in chapel, could that
%   be exploited ?
%- X10 is doing polyhedral optimizations, XL compilers are also using it
%   if there is a possibility of closer cooperation
%
%- Polyhedral - parameterization of linearization, the problems that 
%   are coming out are really really hard. You need an affine, most
%   machines cannot be modeled, so then you need hacks. It is
%  a nice mathematical model but not the panacea. 
%- Limitations - Associavity 
%
%- discussion goes back to imperative Vs declarative 
%   with polyhedral model - extracting the representation 
%   from the source code, or you can write into a DSL and emit
%  the source. Once you have lifted away from having to extract
%  the model why constrain ?
%
%- This is because the community knows how to solve the convex
%   optimization problems but not much as. 
%- The other side is correctness, the limiting factor. 
%
%- Calculus of problem transformation, separate from the actual
%  optimization. Which transformation to apply to what optimization
%  not too many are optimizations are applied all at once.
%- Restrictions imposed by the 
%  polyhedral model, interesting cases is when the subscript case
%  is indirection - is the meta-data becomes bigger than the data
%  so it has to done at runtime
%
%- Automatic compiler techniques - have them for years auto-parallelization
%   we gave up on it, so are we going back to it.
%
%- Data locality -- instead of expressing where the data is, but express
%   implicitly the relation and hierarchy, sufficiently broad possibilities
%   so that it can support a range of back-ends. 
%- In stencil explicitly specify the shape of the stencil, sometimes 
%   you can generate automatic parallelization
%- Right container libraries or constructs will be helpful
%
%- Robustness--if you are counting on the compiler to do auto-parallelization
% for you it needs to be robust. 
%- Data movement implied by the parallelism,
%  what the data access footprint of the loop is. If you break that, the compiler
%  should be able to tell you. Column major Vs row major, if there is a transpose
%  the profiler should point out. Anamoly detection.
%- Expressing locality should be portable, pushing that from applications space
%   to the runtime space. May be unrealistic to push it to the compiler space.
%- Whether all the things should be visible in the syntax of the data structure
%  nice for analysis, the flip side is lack of portability. Ability to reason about
%  the locality in semantic ways is more important, ability to put in assertions
%   and then tools. 
%- Locality and profiler -- how much can a compiler do
%
%- whether your code expresses its belief about its performance characteristics
%- semantic information conveyed to the system could be statically
%   analyzed, the compiler may make the decision to utilize it.
%- The ability to express in a language may be exploited first at the runtime,
%   rather than the compiler.
%- Compositionality - communication and the locality depends on the context.
%
%
%Didem:
%
%Language and Compiler Support for Data Locality 
%
%Why a language rather than a library?
%Brad: fully adapted, pursing lanauge. Keep trieving a lanaugeg because 
%in the long term people want lanauge, syntax matters a lot 
%hard to do in a library, optimization opportunies for compiler 
%
%Amir: Titanium, no one used it. Things are different when vendor supports. 
%Users think that the language 
%Language provides many opportunies 
%Embedded DSL and metaprogramming is a good alternative. 
%We can get most of the lanauge provides. 
%
%Paul: both his DSL are libraries. not language 
%strategically he has to engage with app ppl. It is important to work the app ppl. 
%I am agnostic about how it is deliver to you(compiler). How you get it 
%it is not important. Delay execution
%
%Armind: barrier to adopting a new language is hard 
%syntax extensions are possible 
%
%Cj: go after users? if you want to be portable
%John: if you don't have a common back 
%
%Amir: src-to-src translators address that
%
%Naoya: compiler doesn't mean all the way from high to asembly 
%but to some intermediate form 
%final output is left to the vendor compiler 
%
%Brad: there is a true hunger in users. Users do not have a choice 
%because it is hard because it is no user group, we shouldn't develop a new langa
%that's wrong , that's a dissoppioiting excuse.
%
%Peter: about CUDA, we have 1M downloads. Few syntactic changes, and people 
%are complaining. wondering about timing 
%
%Brad: why java and python are new language and they got adopted. 
%great interoperability 
%
%Didem: complaint about CUDA is not the extension but optimizations 
%because it requires a lot of rewrite of the code 
%
%Carter: entire tool chain change required for CUDA
%
%Anshu: interoperatability is important for a new language because 
%you have to rewrite entire your code 
%
%Brad: he agress interoperatbility. 
%
%John: DSL and lang targetting LLVM  
%is the question what is missing in common backend to inhibits your locality optimizations?
%what is missing in LLVM? or IR?
%
%Brad: targetting C, it is portable assembly 
%multidimensional arrays are missing. C++ next standard is targetting multidim 
%arrays, that will be great. 
%
%Amir: targetting C and C++, lack of any locality information are missing 
%C++11 has some model for parallelism. 
%
%Didem: How to influence C++ committee to address HPC needs?
%
%Carter: join ISO first. pay C++17 ISO join fee
%certain number of meetings to earn voting 
%
%....
%
%...
%
%Armid: annotation and better langauge can provide more info to polyhedral
%LLVM is too low level close to assembly with type info 
%
%...
%
%Kathy: IBM is now working with polyhedral and it is actually XL compiler 
%
%John: algebraic model, data model theory?
%
%Torsten: only applicable to DSL, to fine domain. 
%parameterized integer programming is not pragtical.
%it not a panacea
%
%Thomas: users need something different than imperative langauge 
%python 
%pollyhedral models fall back into 
%
%Torsten: you can extract the poly representation from the source, that source can be 
%anything. convex optimizations, affine functions are easier to solve in the compiler. 
%So it is good that ployhedral constrains your source 
%
%Armind: preserve the correctness of the program, that's the limiting factor 
%the user shouldn't see what's going on 
%
%Padua: optimization process is different that poly 
%
%Armind: objective function 
%we apply opitmizations in order, first parallelization, then locality and then tiling etc. 
%
%...
%
%Paul: people are talking about restrictions of ploy. that's actually great 
%indirection is interesting, metadata that charactizing the iteration space 
%that has to be done in runtime 
%
%
%John: How do we make the data locality and data movement in a program robustly evident in the code?
%what additional information do you need from programmer (model)?
%We gave up on auto parallelization and auto data locality.
%
%Peter: explicitly declared the proximity of the data in Pylaxn
%
%Naoya: in a stencil case, explicitly define stencil shape. COmpiler can generate optimized code.
%even for distributed memory. 
%
%Armind: right data structure 
%
%Paul: 
%1) robustness: in a good day, it gives good performance but always but you need robustness. 
%When the user breaks he knows. 
%
%2) Data movement: how data is distributed
%data access footprint 
%sometimes you need to even declare it 
%
%3) loop 1 and loop 2 : between the two, all to all broadcast. 
%suddenly, it
%profiler should tell you about this bottleneck
%
%Amir: data locality in a portable manner 
%Peter had the right idea. push it from the application to runtime. 
%Pushing to the compiler is too hard. Runtime can do better job. 
%
%Brad: Making the movement explicit 
%it doesn't need to be syntactically 
%all the communication is exploict in the langauge. 
%It had a drawback. because it is portable
%Syntaxtically in Chapel we avoided that. 
%Semantically expressing data locatity is the way to go not syntaxtically 
%
%Padua: how much can compiler do? 
%it is temporaily in the language until the compiler 
%
%Paul: performance properties of your code. If I break your assumptions, 
%I want to 
%compiler fails without an explanation. 
%
%Satoshi: semantic info can convey should be something should 
%be dynamic, hand to the runtime. Some of them can be statically analzyed in the compiler. 
%
%JOhn: HPF had the mistake that putting everything to the compiler. 
%
%Paul: composition: locality depends on the context 

\section{Introduction}

{\it
SCOPE
  \begin{itemize}
  \item In this chapter, we discuss language concepts for data locality, their delivery in general-purpose languages, domain-specific languages and active libraries, and the compiler technology to support them.
  \item We explore the proposition that management of locality in parallel programming is a \emph{language} problem.
  \item Language-based solutions may come in the form of new, general-purpose parallel programming languages.  As well as supporting intuitive syntax, doing so creates the scope for ambitious compilation techniques, sophisticated use of type systems, and compile-time checking of important program properties.  
  \item Language concepts for locality can also be delivered within existing languages, supported in libraries, through metaprogramming in C++, and in ``active libraries" with library-specific analysis and optimizations.
  \item Locality is about data, and locality abstractions refer to abstract data types.  Multidimensional arrays are a crucial starting point.  Many more complex domains can be characterized by the abstract, distributed data structure on which parallel computation occurs.  Domain-specific tools are often focussed on particular data structures - graphs, meshes, octrees, structured adaptive-mesh multigrids etc.  While Chapter~\ref{ch:datastructures} tackles particular data structures, in this chapter we look at how to build tools to support programmers who build such abstractions.
  \end{itemize}
}

Locality is a fundamental concern for current and future HPC architectures, since the cost of data movement defines a large fraction of overall application performance. The concept of locality, however, has not yet become a first-class citizen in general-purpose programming languages. As a result, although application programmers are often confronted with the necessity of restructuring program codes to better exploit locality inherent in their algorithms, even a simple simulation code can become highly non-intuitive, difficult to maintain, and non portable across diverse architectures. 

The overarching vision of this workshop is to solve these issues by presenting application programmers with proper abstractions for expressing locality. In this chapter, we explore the proposition that management of locality in parallel programming is a \emph{language} problem. More specifically, we discuss language concepts for data locality, their delivery in general-purpose languages, domain-specific languages and active libraries, and the compiler technology to support them. 
 
Language-based solutions may come in the form of new, general-purpose parallel
programming languages.  In addition to supporting intuitive syntax, language-based approaches enable ambitious compilation techniques, sophisticated use of type systems, and compile-time checking of important program properties. Language concepts for locality can also be delivered within existing languages, supported in libraries, through metaprogramming in C++, and in ``active libraries'' with library-specific analysis and optimizations.
 

Locality is about data, and locality abstractions often refer to abstract data types. Multidimensional arrays are a crucial starting point.  Many more complex domains can be characterized by the abstract, distributed data structures on which parallel computation occurs. Domain-specific tools are often focused on particular data structures---graphs, meshes, octrees, structured adaptive-mesh multigrids, etc.  While Chapter~\ref{ch:datastructures} tackles particular data structures, in this chapter we look at how to build tools to support programmers who build such abstractions.

Locality is also about affinity---the placement of computations (e.g.,
tasks) relative to the data they access.  Dominant HPC programming
models have typically been based on long-running tasks executing in
fixed locations fetching remote data.  Emerging architectures may also
compel us to pursue languages in which computations are moved to the
data they wish to access.  To implement such models, languages will
need to also support the creation of remote tasks or activities.

The following terms are used in this chapter:

\begin{itemize}
\item \emph{Active library:} a library which comes with a mechanism for delivering library-specific optimizations~\cite{DBLP:journals/corr/math-NA-9810022}.  Active library technologies differ in how this is achieved - examples include template metaprogramming in C++, Lightweight Modular Staging in Scala~\cite{DBLP:journals/cacm/RompfO12}, source-to-source transformation (for example using tools like ROSE~\cite{DBLP:conf/lcpc/YiQ04}), and run-time code generation, driven either by delayed evaluation of library calls~\cite{DBLP:journals/scp/RussellMKB11}, or explicit run-time construction of problem representations or data flow graphs.
\item \emph{Embedded domain-specific language:} a technique for delivering a language-based solution within a host, general-purpose language~\cite{Hudak96buildingdomain-specific}.  Active libraries often achieve this to some extent, commonly by overloading to capture expression structure.  Truly syntactic embedding is also possible with more ambitious tool support~\cite{Erdweg:2011:SLS:2076021.2048099}.
\item \emph{Directive-based language extensions:}

\item \emph{Global-view vs. Local-view Languages:} Global-view
  languages are those in which data structures, such as multidimensional
  arrays, are declared and accessed in terms of their global problem
  size and indices, as in shared-memory programming.  In contrast,
  local-view languages are those in which such data structures are
  accessed in terms of local indices and node IDs.

\item \emph{Multiresolution Language Philosophy:} This is a concept
  in which programmers can move from more language features that are
  more declarative, abstract, and higher-level to those that are more
  imperative, control-oriented, and low-level, as required by their
  algorithm or performance goals.  The goal of this approach is to
  support higher-level abstractions for convenience and productivity
  without removing the fine-grained control that HPC programmers often
  require in practice.  Ideally, the high-level features are
  implemented in terms of the lower-level ones in a way that permits
  programmers to supply their own implementations.  Such an approach
  supports a separation of roles in which computational scientists can
  write algorithms at high levels while parallel computing experts can
  tune the mappings of those algorithms to the hardware platform(s) in
  distinct portions of the program text.



\end{itemize}

\section{Key Points}

{\it
KEY POINTS
  \begin{itemize}
  \item We need to distinguish between different levels of abstraction, and automation - we need solutions for explicit programmer control over locality and data movement, and we need tools to support composition of parallel components in a way that allows locality and communication to be handled automatically.
  \item We need locality and communication to be robustly evident in the source code, so that programmers have a clear model of execution costs that can be supported by profiling tools.
  \item Language-concepts can support programmers in thinking cleanly about locality (such as distinguishing between local-view and global-view).
  \item Expression of locality needs to be portable across machines.
  \end{itemize}
}
\section{State of the Art}

{\it
EXAMPLES OF KEY WORK IN THE AREA
  \begin{itemize}
  \item HPF (and Brad’s blog) - vagueness - robust, well-defined, well-understand abstractions 
  \item UPC (need to have clear ambitions/capability beyond the low-hanging fruit)
  \item Research vehicles can be narrow but a basis for a standard has to have potential for beyond
  \item Titanium, Global arrays
  \item Chapel (multiresolution concept)
  \end{itemize}

STANDARDS
  \begin{itemize}
  \item Opportunities for standardisation of mature technologies
  \item What is this standardisable low-hanging fruit?
  \item how to influence standards?
  \end{itemize}

}

HPF and ZPL are two languages from the 1990s that support high-level
locality specifications through the distribution of multidimensional
arrays and index sets to rectilinear views of the target processors.
Both can be considered \emph{global view} languages, and as a result
all communication was managed by the compiler and runtime.  A key
distinction between the languages was that all communication in ZPL
was syntactically evident, while in HPF it was invisible.  While ZPL's
approach made locality simpler for a programmer to reason about, it
also required code to be rewritten whenever a local/non-distributed
data structure or algorithm was converted to a distributed one.  HPF's
lack of syntactic communication cues saved it from this problem, but
it fell afoul of others in that it did not provide a clear semantic
model for how locality would be implemented for a given program,
requiring programmers to wrestle with a compiler to optimize for
locality, and to then to rewrite their code when moving to a second
compiler that took a different approach.

As we consider current and next-generation architectures, we can
expect the locality model for a compute node to differ from one vendor
or machine generation to the next.  For this reason, the ZPL and HPF
approaches are non-viable.  To this end, we advocate for pursuing
languages that make communication syntactically invisible (to avoid
ZPL's pitfalls) while supporting a strong semantic model as a contract
between the compiler and programmer (to avoid HPF's).  Ideally, this
model would be reinforced by execution-time queries to support
introspection about the placement of data and tasks on the target
architecture.

Chapel is an emerging language that takes this prescribed approach,
using a first-class language-level feature, the \emph{locale} to
represent regions of locality in the target architecture.  Programmers
can reason about the placement of data and tasks on the target
architecture using Chapel's semantic model, or via runtime queries.
Chapel follows the Partitioned Global Address Space~(PGAS) philosophy,
supporting direct access to variables stored on remote locales based
on traditional lexical scoping rules.  Chapel also follows the
multiresolution philosophy by supporting low-level mechanisms for
placing data or tasks on specific locales, as well as high-level
mechanisms for mapping global-view data structures or parallel loops
to the locales.  Advanced users may implement these data distributions
and loop decompositions within Chapel itself, and can even define the
model used to describe a machine's architecture in terms of locales.

Unified Parallel C~(UPC) and Co-Array Fortran~(CAF) are two of the
founding PGAS languages.  UPC supports global-view data structures and
syntactically-invisible communication while CAF has local-view data
structures and syntactically-evident communication.  Both differ from
HPF, ZPL, and Chapel in that programs are written using an explicit
Single-Program, Multiple Data~(SPMD) execution model.  These copies of
the executing binary form the units of locality within these
languages, and remote variable instances are referenced based on the
symmetric namespaces inherent in the SPMD model.  There are good
reasons to be skeptical about whether such SPMD-based locality models
(including MPI) will be sufficient for leveraging the hierarchical
architectural locality present in emerging architectures.  If not,
such models will likely be forced to be part of hybrid programming
models in which distinct locality abstractions are used to express
finer-grained locality concerns.




\section{Discussions}

{\it
AGREEMENTS
\begin{itemize}
\item PRINCIPLES
  \begin{itemize}
  \item Avoid losing information through premature ``lowering" of the program representation.
  \item Isolate cross-cutting locality concerns.  Locality --- data layout and distribution --- is fundamentally more difficult than parallelization because it affects all the code that touches the data.   
  \end{itemize}
\item SOLUTIONS
  \begin{itemize}
  \item There is a lot of consensus around multidimensional data/partitioning/slicing, and how to iterate over them (generators, iterators) - parameterisation of layouts.
  \item There is potential to expose polyhedral concepts to the programmer/IR, and to pass them down to the backend (eg Chapel’s domains, mappings)
  \item The back-end model for the target for code generation and implementation of models/compilers is “missing” - for portable implementation, for interoperability
  \item While we can do a lot with libraries and template metaprogramming, there is a compelling case for a language-based approach
  \end{itemize}
\end{itemize}

DISAGREEMENTS
  \begin{itemize}
   \item No consensus on the requirements on intermediate representations and runtime systems
   \item Do what I say vs do what I mean [[PK: I thnk this is \emph{agreement} on the need for "multiresolution" solutions, see earlier]]
   \item Compositional metadata vs explicit dependence/synchronisation/movement (Brad has words)
  \end{itemize}


  GAPS (what is missing? not covered at the workshop)
  \begin{itemize}
   \item
   \end{itemize}

}
\section{Research Plan}

{\it
RESEARCH AGENDA
  \begin{itemize}
  \item Scope out the opportunities for JIT - plenty of compelling need for infrastructure (specialisation, autotuning, …) [[what’s the research agenda/challenge here?]]
  \item How do we create a user community?
  \end{itemize}

CONCLUSION - RECOMMENDATIONS
  \begin{itemize}
  \item Opportunities for standardisation of mature technologies
  \item What is this standardisable low-hanging fruit?
  \item How to influence standards?
  \item Define research agenda
  \end{itemize}
}


% LocalWords:  HPC
